import torch
from torch import nn
import random
import math
import torch
import torch.distributed as dist
from classy_vision.generic.distributed_util import (
    convert_to_distributed_tensor,
    convert_to_normal_tensor,
    is_distributed_training_run,
)


class GatherLayer(torch.autograd.Function):
    """
    Gather tensors from all workers with support for backward propagation:
    This implementation does not cut the gradients as torch.distributed.all_gather does.
    """

    @staticmethod
    def forward(ctx, x):
        output = [torch.zeros_like(x) for _ in range(dist.get_world_size())]
        dist.all_gather(output, x)
        return tuple(output)

    @staticmethod
    def backward(ctx, *grads):
        all_gradients = torch.stack(grads)
        dist.all_reduce(all_gradients)
        return all_gradients[dist.get_rank()]


def gather_from_all(tensor: torch.Tensor) -> torch.Tensor:
    """
    Similar to classy_vision.generic.distributed_util.gather_from_all
    except that it does not cut the gradients
    """
    if tensor.ndim == 0:
        # 0 dim tensors cannot be gathered. so unsqueeze
        tensor = tensor.unsqueeze(0)

    if is_distributed_training_run():
        tensor, orig_device = convert_to_distributed_tensor(tensor)
        gathered_tensors = GatherLayer.apply(tensor)
        gathered_tensors = [
            convert_to_normal_tensor(_tensor, orig_device)
            for _tensor in gathered_tensors
        ]
    else:
        gathered_tensors = [tensor]
    gathered_tensor = torch.cat(gathered_tensors, 0)
    return gathered_tensor




class args_clr:
    mode = "directclr"
    dim = 128
    label = None
    d_model = 512

class directCLR(nn.Module):
    """
    Paper: https://arxiv.org/pdf/2110.09348.pdf
    Adapt from https://github.com/facebookresearch/directclr/tree/main

    """
    def __init__(self, args=args_clr):
        super().__init__()
        self.args = args
        if  args.label is not None:
            # self.backbone = torchvision.models.resnet50(zero_init_residual=True)
            # self.backbone.fc = nn.Identity()
            self.online_head = nn.Linear(2048, args.label)

        if self.args.mode == "simclr":
            sizes = [2048, 2048, 128]
            layers = []
            for i in range(len(sizes) - 2):
                layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))
                layers.append(nn.BatchNorm1d(sizes[i+1]))
                layers.append(nn.ReLU(inplace=True))
            layers.append(nn.Linear(sizes[-2], sizes[-1], bias=False))
            layers.append(nn.BatchNorm1d(sizes[-1]))
            self.projector = nn.Sequential(*layers)
        elif self.args.mode == "single":
            self.projector = nn.Linear(2048, 128, bias=False)

        self.relu = nn.ReLU(inplace=True)

    def forward(self, r1, r2, labels=None):
        # r1 = self.backbone(y1)
        # r2 = self.backbone(y2)

        if self.args.mode == "baseline":
            z1 = r1
            z2 = r2
            loss = infoNCE(z1, z2) / 2 + infoNCE(z2, z1) / 2
        elif self.args.mode == "directclr":
            z1 = r1[:, :self.args.dim]
            z2 = r2[:, :self.args.dim]
            loss = infoNCE(z1, z2) / 2 + infoNCE(z2, z1) / 2
        elif self.args.mode == "group":
            idx = np.arange(2048)
            np.random.shuffle(idx)
            loss = 0
            for i in range(8):
                start = i * 256
                end = start + 256
                z1 = r1[:, idx[start:end]]
                z2 = r2[:, idx[start:end]]
                loss = loss + infoNCE(z1, z2) / 2 + infoNCE(z2, z1) / 2
            
        elif self.args.mode == "simclr" or self.args.mode == "single":
            z1 = self.projector(r1)
            z2 = self.projector(r2)
            loss = infoNCE(z1, z2) / 2 + infoNCE(z2, z1) / 2

        if labels is not None:
            logits = self.online_head(r1.detach())
            cls_loss = torch.nn.functional.cross_entropy(logits, labels)
            acc = torch.sum(torch.eq(torch.argmax(logits, dim=1), labels)) / logits.size(0)

            loss = loss + cls_loss

        return loss

def infoNCE(nn, p, temperature=0.1):
    nn = torch.nn.functional.normalize(nn, dim=1)
    p = torch.nn.functional.normalize(p, dim=1)
    nn = gather_from_all(nn)
    p = gather_from_all(p)
    logits = nn @ p.T
    logits /= temperature
    n = p.shape[0]
    labels = torch.arange(0, n, dtype=torch.long).cuda()
    loss = torch.nn.functional.cross_entropy(logits, labels)
    return loss
